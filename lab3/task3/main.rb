#!/usr/bin/env ruby
# –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞ —Ä–æ–±–æ—Ç–∞: –°–∫–∞–Ω–µ—Ä –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ —É —Ñ–∞–π–ª–æ–≤—ñ–π —Å–∏—Å—Ç–µ–º—ñ
# –í–∏–∫–æ–Ω—É—î —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–∏–π –æ–±—Ö—ñ–¥ –∫–∞—Ç–∞–ª–æ–≥—É, –ø–æ—à—É–∫ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ —Ç–∞ —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è duplicates.json

require 'digest'
require 'json'
require 'find'

# === –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è ===
ROOT_DIR = File.join(__dir__, 'task3_data')
IGNORE_DIRS = ['.git', 'node_modules', '__pycache__'] # –Ü–≥–Ω–æ—Ä–æ–≤–∞–Ω—ñ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó
REPORT_FILE = File.join(ROOT_DIR, 'duplicates.json')

# === –ó–±—ñ—Ä —É—Å—ñ—Ö —Ñ–∞–π–ª—ñ–≤ ===
def collect_files(root, ignore_dirs)
  files = []
  Find.find(root) do |path|
    if File.directory?(path)
      if ignore_dirs.any? { |d| File.basename(path) == d }
        Find.prune
      end
      next
    end
    next unless File.file?(path)
    files << { path: path, size: File.size(path) }
  end
  files
end

# === –ì—Ä—É–ø—É–≤–∞–Ω–Ω—è –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏—Ö –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ ===
def group_by_size(files)
  files.group_by { |f| f[:size] }.select { |_size, arr| arr.size > 1 }
end

# === –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ –∑–∞ —Ö–µ—à–µ–º ===
def group_by_hash(groups)
  new_groups = []
  groups.each do |size, files|
    hash_groups = files.group_by do |f|
      Digest::SHA256.file(f[:path]).hexdigest rescue nil
    end
    hash_groups.each_value do |hgroup|
      new_groups << { size_bytes: size, files: hgroup.map { |f| f[:path] } } if hgroup.size > 1
    end
  end
  new_groups
end

# === –ü–æ–±–∞–π—Ç–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ ===
def byte_compare(f1, f2)
  File.open(f1, 'rb') do |a|
    File.open(f2, 'rb') do |b|
      until a.eof? || b.eof?
        return false unless a.read(4096) == b.read(4096)
      end
    end
  end
  true
end

def confirm_duplicates(groups)
  confirmed = []
  all_files = groups.flat_map { |g| g[:files] }
  unique_files = []

  groups.each do |g|
    unique_in_group = []
    g[:files].each do |file|
      if unique_in_group.none? { |u| byte_compare(u, file) }
        unique_in_group << file
      end
    end
    if unique_in_group.size < g[:files].size
      confirmed << {
        size_bytes: g[:size_bytes],
        saved_if_dedup_bytes: g[:size_bytes] * (g[:files].size - unique_in_group.size),
        files: g[:files]
      }
      unique_files.concat(unique_in_group)
    end
  end

  # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ confirmed —ñ –æ–±—á–∏—Å–ª—é—î–º–æ –∑–∞–≥–∞–ª—å–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤
  [confirmed, all_files.size - unique_files.uniq.size]
end

# === –û—Å–Ω–æ–≤–Ω–∞ –ª–æ–≥—ñ–∫–∞ ===
puts "üîç –°–∫–∞–Ω—É–≤–∞–Ω–Ω—è –∫–∞—Ç–∞–ª–æ–≥—É: #{ROOT_DIR}..."
files = collect_files(ROOT_DIR, IGNORE_DIRS)
puts "üìÅ –ó–Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª—ñ–≤: #{files.size}"

size_groups = group_by_size(files)
puts "üì¶ –ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏—Ö –≥—Ä—É–ø –∑–∞ —Ä–æ–∑–º—ñ—Ä–æ–º: #{size_groups.size}"

hash_groups = group_by_hash(size_groups)
puts "üîë –ì—Ä—É–ø –ø—ñ—Å–ª—è —Ö–µ—à—É–≤–∞–Ω–Ω—è: #{hash_groups.size}"

confirmed_groups, total_duplicates = confirm_duplicates(hash_groups)
puts "‚úÖ –ü—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–æ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤: #{total_duplicates}"

# === –§–æ—Ä–º—É–≤–∞–Ω–Ω—è –∑–≤—ñ—Ç—É ===
report = {
  scanned_files: files.size,
  groups: confirmed_groups
}

confirmed_groups.each_with_index do |group, i|
  puts "\n–ì—Ä—É–ø–∞ ##{i + 1}:"
  puts "  –†–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—ñ–≤: #{group[:size_bytes]} –±–∞–π—Ç"
  puts "  –î—É–±–ª—ñ–∫–∞—Ç—ñ–≤: #{group[:files].size - 1}" # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ —É –≥—Ä—É–ø—ñ
  puts "  –§–∞–π–ª–∏:"
  group[:files].each { |f| puts "    - #{f}" }
end

File.write(REPORT_FILE, JSON.pretty_generate(report))
puts "\n–ó–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É #{REPORT_FILE}"